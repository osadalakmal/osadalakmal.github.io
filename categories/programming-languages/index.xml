<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Programming Languages on Osada Blog – Thoughts on Software Engineering, Programming, Systems, and Life</title>
        <link>https://osada.blog/categories/programming-languages/</link>
        <description>Recent content in Programming Languages on Osada Blog – Thoughts on Software Engineering, Programming, Systems, and Life</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-gb</language>
        <lastBuildDate>Sun, 17 Aug 2025 21:50:31 +0100</lastBuildDate><atom:link href="https://osada.blog/categories/programming-languages/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>The Shipping Forecast and the Art of Communication</title>
        <link>https://osada.blog/posts/be-more-like-shipping-forecast/</link>
        <pubDate>Sun, 17 Aug 2025 21:50:31 +0100</pubDate>
        
        <guid>https://osada.blog/posts/be-more-like-shipping-forecast/</guid>
        <description>&lt;img src="https://osada.blog/posts/be-more-like-shipping-forecast/shipping.webp" alt="Featured image of post The Shipping Forecast and the Art of Communication" /&gt;&lt;p&gt;As an immigrant to the UK, I had to absorb the culture as a grown-up. Things that others learned through cultural osmosis, I had to consciously seek out and understand - including the norms of UK TV and radio. Now that I&amp;rsquo;m working on BBC Sounds, it&amp;rsquo;s more important than ever for me to be aware of the cultural touchstones of Radioland.&lt;/p&gt;
&lt;p&gt;For a long time, I had no idea what the Shipping Forecast was, although it seemed as quintessentially British as it could get. I only knew it from the “Calm” playlists on BBC Sounds - a chant-like recital of odd place names and numbers that people found oddly soothing.&lt;/p&gt;
&lt;p&gt;Then one day, exhausted and a little breathless after a hike, I caught the Shipping Forecast live on Radio 4. For the first time, I truly listened. Its rhythm was regular, clipped, steady - strangely calming. Most of it was guessable: numbers for wind force, the last part for visibility. But the place names were a mystery. My curiosity was piqued.&lt;/p&gt;
&lt;p&gt;What struck me when I dug deeper was how deliberately it was designed - not for poetry, but for survival. A century on air, the Shipping Forecast continues to save sailors’ lives because of the way it communicates: clearly, consistently, and unambiguously in a noisy environment. And that, I realized, is exactly the lesson we need for technical communication in our own teams and organizations.&lt;/p&gt;
&lt;h2 id=&#34;a-crash-course-on-the-shipping-forecast&#34;&gt;A Crash Course on the Shipping Forecast
&lt;/h2&gt;&lt;p&gt;The Shipping Forecast is a weather bulletin for the seas around the British Isles. Issued by the UK Met Office, it has been on the air since 1924. Its origins trace back to Vice-Admiral Robert FitzRoy in the 1860s, who began issuing storm warnings to reduce shipwrecks.&lt;/p&gt;
&lt;h3 id=&#34;structure-of-the-forecast&#34;&gt;Structure of the Forecast
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;General Synopsis&lt;/strong&gt; - A summary of pressure systems (highs and lows), their current location, pressure in millibars, and their expected movement.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example: “Low, Rockall, 990, slow-moving, expected 1002 by 0600 tomorrow.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sea Area Forecasts&lt;/strong&gt; - 31 named sea areas around the UK and Ireland, always read in the same order.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;each-areas-forecast-includes&#34;&gt;Each Area’s Forecast Includes
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Wind direction and strength (Beaufort scale, 0-12)&lt;/li&gt;
&lt;li&gt;Weather (e.g., rain, showers, fog, fair)&lt;/li&gt;
&lt;li&gt;Visibility (good = &amp;gt;5 miles, moderate = 2-5 miles, poor = 1-2 miles, very poor = &amp;lt;1 mile)&lt;/li&gt;
&lt;li&gt;Optional tidal notes in extended bulletins&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-facts&#34;&gt;Key Facts
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;First broadcast: 1924 on the BBC.&lt;/li&gt;
&lt;li&gt;Still broadcast four times daily on Radio 4 (0048, 0520, 1201, 1754).&lt;/li&gt;
&lt;li&gt;It covers 31 sea areas, such as Viking, Dogger, German Bight, Fastnet, and Rockall.&lt;/li&gt;
&lt;li&gt;The language is deliberately short, standardized, and unambiguous for clarity on poor radios.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;when-the-seas-are-rough-organizational-communication&#34;&gt;When the Seas Are Rough: Organizational Communication
&lt;/h2&gt;&lt;p&gt;I’ve experienced firsthand how crucial communication becomes during upheaval. When an organization goes through a major transformation, communication is not an afterthought-it&amp;rsquo;s the glue that holds everything together. As leaders, how we handle this communication is paramount, because the success of the transformation depends on it. The people we manage look to us to provide information and reassurance that the disruptions and changes are truly for the good of the organization. They want to know how it will impact them, and what tomorrow will look like.&lt;/p&gt;
&lt;p&gt;And yet, too often, communication during transformations feels overbearing. Updates are packed with details people don’t need or aren’t equipped to decipher. The essentials get buried, and people tune out. In tuning out, they miss the pieces of information that could genuinely help them navigate change. Even more harmful, they may interpret a lack of clarity and superfluous details as an intentional effort to obfuscate. This quickly erodes trust, making it harder to get positive buy-in for the changes we are implementing.&lt;/p&gt;
&lt;p&gt;This has always frustrated me. We need a framework that keeps communicators focused on what matters most - the salient details, presented consistently, with no ambiguity. And the Shipping Forecast offers exactly that model.&lt;/p&gt;
&lt;h2 id=&#34;the-problem-noisy-inefficient-communication&#34;&gt;The Problem: Noisy, Inefficient Communication
&lt;/h2&gt;&lt;p&gt;In the situations that matter most - organizational updates, incident response, migrations - the goal is simple: provide enough information to give employees the confidence they need. But communication often fails in predictable ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Updates ramble and lose people’s attention. For example, an incident update that drags on for ten minutes without ever stating the impact or action items leaves everyone confused.&lt;/li&gt;
&lt;li&gt;Messages lack structure, so if you join late, you’re lost. A migration briefing that jumps between topics with no clear beginning, middle, or end forces people to guess where they are in the plan.&lt;/li&gt;
&lt;li&gt;Jargon creeps in, introducing ambiguity. Saying “we’ll refactor the orchestration layer” without explaining what systems that refers to leaves many stakeholders unsure of the change.&lt;/li&gt;
&lt;li&gt;Some things are over-explained while others are skipped. A design review might go into detail about which framework version is used but never mention who owns the rollout.&lt;/li&gt;
&lt;li&gt;Updates arrive inconsistently, so no one knows when to expect them. A status email sent at random times each week is quickly ignored because the team cannot rely on it as a dependable source of truth.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The result is confusion, frustration, and wasted energy. In moments of transformation or crisis, the cost is even higher: teams feel unmoored.&lt;/p&gt;
&lt;h2 id=&#34;what-the-shipping-forecast-gets-right&#34;&gt;What the Shipping Forecast Gets Right
&lt;/h2&gt;&lt;p&gt;The Shipping Forecast has perfected communication for noisy, high-stakes environments. Its principles are deceptively simple but powerful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: Always in the same order, with the same structure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Brevity&lt;/strong&gt;: Nothing is superfluous; only the essentials remain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clarity&lt;/strong&gt;: Standardized words ensure there is no ambiguity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coverage&lt;/strong&gt;: The whole coastline is covered, end to end.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reliability&lt;/strong&gt;: Delivered four times daily, without fail.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Repetition&lt;/strong&gt;: Core facts are repeated until they stick.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For sailors, these principles are life-saving. For us, they can make the difference between communication that enables action and communication that creates noise.&lt;/p&gt;
&lt;h2 id=&#34;why-consistency-coverage-and-reliability-matter&#34;&gt;Why Consistency, Coverage and Reliability Matter
&lt;/h2&gt;&lt;p&gt;In organizational transformations, consistency and reliable repetition aren’t dull-they build confidence. People are dealing with new ways of working, new technology, and new structures. In that environment, clear consistent repetition is reassurance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It shows nothing is being hidden.&lt;/li&gt;
&lt;li&gt;It signals that everything has been thoroughly thought through.&lt;/li&gt;
&lt;li&gt;It makes the audience confident in the plan because they can see its completeness.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For engineering managers and tech leads, this matters directly. Your team should come to believe that communication is crafted for their benefit, that it was intentional and thoughtful, and that leadership cares about making the change succeed.&lt;/p&gt;
&lt;h2 id=&#34;how-to-make-your-communication-more-like-the-forecast&#34;&gt;How to Make Your Communication More Like the Forecast
&lt;/h2&gt;&lt;p&gt;When clarity matters most - for organizational updates, technical changes, incidents, or migrations - we should borrow from the Shipping Forecast:&lt;/p&gt;
&lt;h3 id=&#34;consistency&#34;&gt;Consistency
&lt;/h3&gt;&lt;p&gt;Use the same structure every time. For example, always start incident updates with impact, then mitigation, then next steps. Likewise, if you write weekly status reports, keep the same headings so readers know where to look for information.&lt;/p&gt;
&lt;h3 id=&#34;brevity&#34;&gt;Brevity
&lt;/h3&gt;&lt;p&gt;Cut the noise; provide only what matters. An update that simply says, “Service X is degraded, engineers are working on rollback, ETA 20 minutes” is far more effective than three paragraphs of background detail. A migration notice that summarizes the change in three bullet points is more likely to be read than a three-page essay.&lt;/p&gt;
&lt;h3 id=&#34;clarity&#34;&gt;Clarity
&lt;/h3&gt;&lt;p&gt;Avoid ambiguity; use the simplest words. Instead of saying “there are issues with authentication subsystems,” say “users cannot log in.” Instead of “orchestration layer refactor,” say “we are changing how services talk to each other.”&lt;/p&gt;
&lt;h3 id=&#34;coverage&#34;&gt;Coverage
&lt;/h3&gt;&lt;p&gt;Cover the topic end to end. If you announce a new deployment pipeline, don’t just describe the technical flow; also mention who owns it, how teams onboard, and what support is available. Similarly, if you communicate a process change, spell out both what is changing and what stays the same.&lt;/p&gt;
&lt;h3 id=&#34;reliability&#34;&gt;Reliability
&lt;/h3&gt;&lt;p&gt;Deliver updates at a predictable rhythm. Sending a daily stand-up note at 9am sharp builds trust. Publishing a weekly engineering summary every Friday ensures people rely on it. Irregular communication erodes confidence.&lt;/p&gt;
&lt;h3 id=&#34;repetition&#34;&gt;Repetition
&lt;/h3&gt;&lt;p&gt;Reinforce the essentials until they stick. In a transformation, it is worth saying three times: “The old system will be retired on 31 October.” In incident communications, repeat the key action: “Restart your client if you see this error.” Repetition of core facts reassures people that they haven’t missed something.&lt;/p&gt;
&lt;h2 id=&#34;closing&#34;&gt;Closing
&lt;/h2&gt;&lt;p&gt;For a century, the Shipping Forecast has guided sailors through storms. I discovered it by chance and was struck by how soothing it was. But the reason it’s soothing is the same reason it’s effective: it is structured, consistent, reliable, and repetitive.&lt;/p&gt;
&lt;p&gt;Not every kind of communication should be like the Shipping Forecast - some moments demand storytelling, persuasion, or brainstorming. But when the goal is to disseminate information clearly and efficiently, especially in noisy or turbulent environments, the lesson holds.&lt;/p&gt;
&lt;p&gt;Good communication doesn’t just inform. Like the Shipping Forecast, it enables people to act with confidence, no matter how stormy the seas.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Programming&#39;s New Frontier: The Rise of LLM-First Languages</title>
        <link>https://osada.blog/posts/languages-designed-for-llms/</link>
        <pubDate>Thu, 14 Aug 2025 20:16:03 +0100</pubDate>
        
        <guid>https://osada.blog/posts/languages-designed-for-llms/</guid>
        <description>&lt;img src="https://osada.blog/posts/languages-designed-for-llms/designed.webp" alt="Featured image of post Programming&#39;s New Frontier: The Rise of LLM-First Languages" /&gt;&lt;p&gt;Large Language Models (LLMs) have quickly become an integral part of modern software development. Today, most developers encounter them as coding assistants-tools that can generate code on demand by drawing upon patterns learned from vast quantities of open-source and proprietary code. These models can also reference online resources and produce functioning code in seconds.&lt;/p&gt;
&lt;p&gt;But as impressive as they are, this process is far from foolproof.&lt;/p&gt;
&lt;h2 id=&#34;the-problem-with-current-llm-assisted-coding&#34;&gt;The Problem with Current LLM-Assisted Coding
&lt;/h2&gt;&lt;p&gt;While LLMs can produce working solutions quickly, they are prone to significant shortcomings:&lt;/p&gt;
&lt;p&gt;Here are the links for each issue mentioned in the “Current Reality of LLM-Assisted Coding” section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hallucinated dependencies&lt;/strong&gt;: LLMs often generate code that references libraries or APIs that do not exist. This is more than a simple inconvenience; it is now a significant security vulnerability. This phenomenon, sometimes called &amp;ldquo;slopsquatting,&amp;rdquo; creates a new vector for supply chain attacks. Malicious actors can preemptively squat on these hallucinated package names in public repositories like npm and PyPI. &lt;a class=&#34;link&#34; href=&#34;https://www.techradar.com/pro/mitigating-the-risks-of-package-hallucination-and-slopsquatting&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;TechRadar – Mitigating the risks of package hallucination&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Logic errors&lt;/strong&gt;: They can make subtle mistakes that pass undetected until runtime. This is why LLMs are dangerous when used by junior developers. &lt;a class=&#34;link&#34; href=&#34;https://medium.com/@adnanmasood/code-generation-with-llms-practical-challenges-gotchas-and-nuances-7b51d394f588&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium – Code generation with LLMs: practical challenges&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Test manipulation&lt;/strong&gt;: In some cases, they will &amp;ldquo;cheat&amp;rdquo; by altering tests to make broken code pass. &lt;a class=&#34;link&#34; href=&#34;https://medium.com/@aipapers/cheating-llms-how-not-to-stop-them-openai-paper-explained-c38ebc637762&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium – Cheating LLMs: How (not) to stop them&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Context limitations&lt;/strong&gt;: Providing complete context is still a challenge. Developers have experimented with approaches like concatenating entire codebases into a single file, RAG-based (retrieval-augmented generation) solutions, and specialized formats like &lt;code&gt;LLMs.txt&lt;/code&gt;-a file designed to tell the model the &amp;ldquo;story&amp;rdquo; of the repository. Yet none of these methods are optimal. &lt;a class=&#34;link&#34; href=&#34;https://datanorth.ai/blog/context-length&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DataNorth – Context length in LLMs&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-the-time-is-right-for-a-change&#34;&gt;Why the Time is Right for a Change
&lt;/h2&gt;&lt;p&gt;We’ve crossed a Rubicon—a point of no return—beyond which the capabilities of coding assistants have fundamentally changed the nature of what’s possible. While earlier models like GPT-4 offered impressive performance, they often struggled with long-term coherence and could &amp;ldquo;forget&amp;rdquo; earlier parts of a conversation due to a more limited context window. The developer&amp;rsquo;s workflow often involved significant manual effort to re-supply context, summarize past conversations, or break down large tasks into small, manageable chunks.&lt;/p&gt;
&lt;p&gt;With the arrival of models like Claude Opus, Claude Sonnet, and GLM-4.5, this dynamic has shifted. Their primary innovation is not just higher performance on benchmarks, but a qualitative leap in their ability to handle massive context windows—hundreds of thousands of tokens, equivalent to thousands of pages of code and documentation. This allows assistants to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Maintain state over long, multi-file projects: They can now &amp;ldquo;remember&amp;rdquo; an entire codebase or a significant portion of it, enabling them to make changes that are consistent across multiple files without losing track of the overall architecture.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reason over a complete set of documentation: The model can be given an entire project&amp;rsquo;s documentation, API references, or even a full RFC, and use that information to generate code that is correct and adheres to all specified constraints.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exhibit &amp;ldquo;extended thinking&amp;rdquo;: Some of these models are now capable of multi-step, agentic reasoning, where they can generate a plan, execute it, receive feedback, and adjust their strategy, all within a single session. This moves them from being mere code generators to active problem-solvers.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Projects like Lovable demonstrate this shift. The fact that such initiatives can deliver working applications in a single pass is evidence that we’ve moved past the experimental phase into a new era of practical, production-ready LLM-assisted programming.&lt;/p&gt;
&lt;h2 id=&#34;the-case-for-llm-native-languages&#34;&gt;The Case for LLM-Native Languages
&lt;/h2&gt;&lt;p&gt;The history of programming languages is a story of responding to the shortcomings of the tools that came before:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kroki.io/plantuml/svg/eNptkl1v0zAUhu_9K46WG6gWqU3SrtsFWqnKBEqrQSW44eYkPk2sOfZkO2UD8d-x3VAiNt_5PI_Oef1xax0a13eS2QehHtFgBxXWD43RveJrLbUBZ1BZj0i5kYXG6B8nIaniGkFOB-yl-6CV22FHsDIC5et8L34SzJYj6ISTdEZZ9hoaBlNcI2EVUr0URobSnOAXA3j_3zGTLMuW-SwQbTiZoTqdIq_mvvqyK8A9ci5UA9nUb76cehlFBmZz9psxSQcHToMRTeuAC0O1E1qxeAzYHLXswx70Ae6Nbny8LnQrUTU9NmQZi2kvtli3QhGs_e4C0MJ2DUl-nVOxGIyVtdRV8jnS1X4LSUFzvqgHnBsOd6S-qzfrS_iER3wbxbvNLodkzq_yKxrMwrWDWWL19ePm2yXsP5dnvYBkwa-Xy79zJ5Oy3KY7dOJI_3JPJtH36Hx_zEeGNH0Xw90AVtZ_q9NthEogMc0NtP6yyKSSjiTBPiuHTyyiwSm8w3WHQqX2kWpxEDWL5cDDSM9r_1b05FIj6hbCu3PRdJbdkuLhr_8BZ4P21Q==&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Evolution of Programming Languages&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The leap from &lt;strong&gt;machine code&lt;/strong&gt; to &lt;strong&gt;assembly&lt;/strong&gt; abstracted raw binary into human-readable mnemonics.&lt;/li&gt;
&lt;li&gt;The arrival of &lt;strong&gt;third-generation languages&lt;/strong&gt; (C, Java) provided higher abstraction for productivity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fourth-generation languages&lt;/strong&gt; like LabVIEW tackled domain-specific needs.&lt;/li&gt;
&lt;li&gt;The rise of &lt;strong&gt;memory-safe compiled languages&lt;/strong&gt; like Rust and Go directly responded to decades of security vulnerabilities from unsafe memory operations.&lt;/li&gt;
&lt;li&gt;The rise of JVM based alternatives to Java came about because of stagnation in Java roadmap. This gave us languages like kotlin, Closure and Scala.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These shifts happened because the &lt;em&gt;context&lt;/em&gt; in which code was written and executed changed.&lt;/p&gt;
&lt;p&gt;Today, we are at another inflection point. The dominant programming languages-Python, JavaScript, Java, C++-are designed to be read, understood, and authored by humans. LLMs can emulate human thought, but that’s still emulation. There’s no reason to believe that a language designed to be optimal for &lt;em&gt;human&lt;/em&gt; authorship is also optimal for &lt;em&gt;machine&lt;/em&gt; authorship.&lt;/p&gt;
&lt;h2 id=&#34;literate-programming-as-a-precursor&#34;&gt;Literate Programming as a Precursor
&lt;/h2&gt;&lt;p&gt;Donald Knuth’s concept of literate programming [Knuth, 1984] was created so that human-readable descriptions could be embedded alongside code-not merely interspersed, but with documentation as the primary artifact and code as a secondary element. In literate programming, the entire program becomes executable documentation.&lt;/p&gt;
&lt;p&gt;For LLMs, this is a natural fit. LLMs excel when they have rich, continuous context, and literate programming provides exactly that: the whole program and its purpose, rationale, and constraints in one coherent narrative. This makes literate programming an ideal model for feeding an LLM the maximum relevant information. We may see a resurgence of this paradigm, adapted for machine consumers as much as for humans.&lt;/p&gt;
&lt;p&gt;Modern tools inspired by literate programming-such as &lt;a class=&#34;link&#34; href=&#34;https://jupyter.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jupyter Notebooks&lt;/a&gt;, &lt;a class=&#34;link&#34; href=&#34;https://quarto.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Quarto&lt;/a&gt;, &lt;a class=&#34;link&#34; href=&#34;https://www-cs-faculty.stanford.edu/~knuth/noweb.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;noweb&lt;/a&gt;, and &lt;a class=&#34;link&#34; href=&#34;https://nbdev.fast.ai/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;nbdev&lt;/a&gt; already combine narrative and code in ways that could evolve toward LLM-first formats.&lt;/p&gt;
&lt;h2 id=&#34;lessons-from-apl-j-and-q&#34;&gt;Lessons from APL, J, and Q
&lt;/h2&gt;&lt;p&gt;Some languages have historically prioritized other qualities above human readability. APL, J, and Q embrace terse, symbolic syntax for reasons of efficiency and expressiveness. In these languages, code becomes subservient to the goal-whether that’s mathematical compactness or performance-rather than ease of reading.&lt;/p&gt;
&lt;p&gt;Similarly, an LLM-first language may look alien to human eyes, optimized for machine parsing and generation rather than human comprehension. In this way, APL and its descendants offer a blueprint: concise, unambiguous, and structured for the intended consumer, even if that consumer is a machine.&lt;/p&gt;
&lt;h2 id=&#34;characteristics-of-an-llm-first-language&#34;&gt;Characteristics of an LLM-First Language
&lt;/h2&gt;&lt;p&gt;An LLM-native programming language might incorporate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ultra-Explicit Semantics&lt;/strong&gt; – No implicit defaults; strict typing and explicit declarations. No room for ambiguity please, LLMs bring plenty of their own.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Self-Describing Code&lt;/strong&gt; – Embedded, machine-readable metadata describing intent, dependencies, and constraints. i.e. Literate Programming. Or Joe Armstrong style write-docs-before-code philosophy needed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chunk-Optimized Structure&lt;/strong&gt; – Modular design aligned with token window limits for easy context retrieval. Each one would need to be mostly self contained and with the output described separately so it can be fed in to LLMs while the other modules are being generated. Think C/C++ header files.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Error Prevention by Design&lt;/strong&gt; – Syntax rules that block common LLM pitfalls, such as referencing undeclared libraries or similar hallucinations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context Anchoring&lt;/strong&gt; – Persistent IDs and hash-based references to ensure the correct version is always referenced. (Content Addressable Storage anyone?)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stable, Bounded Syntax&lt;/strong&gt; – Predictable token patterns for better compression and embedding performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrated Machine/Compiler Feedback&lt;/strong&gt; – Output structured for both humans and models to consume.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;speculative-syntax-examples&#34;&gt;Speculative Syntax Examples
&lt;/h2&gt;&lt;p&gt;Here’s what such a language could resemble:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-llmcode&#34; data-lang=&#34;llmcode&#34;&gt;@module(meta={&amp;#34;version&amp;#34;:&amp;#34;1.2.0&amp;#34;,&amp;#34;purpose&amp;#34;:&amp;#34;Data ingestion pipeline&amp;#34;})
module ingest_pipeline {

  @function(meta={&amp;#34;context_id&amp;#34;:&amp;#34;hash1234&amp;#34;,&amp;#34;owner&amp;#34;:&amp;#34;team-data&amp;#34;})
  fn load_csv(file_path: String) -&amp;gt; DataFrame {
    ensure(file_exists(file_path))
    return parse_csv(file_path)
  }

  @test_integrity(id=&amp;#34;test-001&amp;#34;, immutable=true)
  fn test_load_csv_valid() {
    df = load_csv(&amp;#34;/test/data.csv&amp;#34;)
    assert(df.rows &amp;gt; 0)
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This speculative syntax:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uses metadata annotations for context.&lt;/li&gt;
&lt;li&gt;Embeds test integrity markers to prevent silent tampering.&lt;/li&gt;
&lt;li&gt;Organizes code in retrieval-friendly modules.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;literate-programming-executable-context-for-llms&#34;&gt;Literate Programming: Executable Context for LLMs
&lt;/h2&gt;&lt;p&gt;Donald Knuth’s &lt;a class=&#34;link&#34; href=&#34;http://www.literateprogramming.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;literate programming&lt;/a&gt; reframes software as &lt;em&gt;executable documentation&lt;/em&gt;: the narrative for humans is primary; the code is woven into that story and then &lt;em&gt;tangled&lt;/em&gt; into compilable units. In a literate system, the whole program &lt;em&gt;is&lt;/em&gt; context-definitions, intent, trade‑offs, and usage are embedded right where they matter.&lt;/p&gt;
&lt;p&gt;That property makes literate programming a strong fit for LLMs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Context density&lt;/strong&gt;: LLMs perform better with rich, proximate context; literate code puts rationale, invariants, and edge cases next to the implementation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Retrieval-friendly structure&lt;/strong&gt;: Sections (&amp;ldquo;chunks&amp;rdquo;) can be addressed by name and dependency, making them ideal targets for RAG and for toolchains that assemble the right context window.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intent and constraints&lt;/strong&gt;: Narratives, design notes, and correctness arguments can be expressed in machine-readable blocks that the model can obey during generation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrity by design&lt;/strong&gt;: Tests and specs live as first-class prose+code; changes to tests are visible in the narrative, discouraging silent “cheats.”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Practical hybrid:&lt;/em&gt; an LLM-first language could standardize doc-first modules, where every module begins with a prose contract (purpose, invariants, failure modes), followed by code blocks tagged for tangling, and a final verification block of examples/tests.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sketch:&lt;/strong&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-litlang&#34; data-lang=&#34;litlang&#34;&gt;# chunk: normalize_emails
purpose: Deduplicate case/alias variants. Invariant: output preserves domain.
assumptions: input ~ list&amp;lt;String&amp;gt;; may contain nulls.

code tangles to email/normalize.llm:
fn norm(xs: List&amp;lt;String&amp;gt;) -&amp;gt; List&amp;lt;String&amp;gt; {
  return xs.filter(not_null)
           .map(lowercase)
           .map(strip_gmail_aliases)  // invariant: keep domain
           .unique()
}

verify:
ex1: norm([&amp;#34;A@x.com&amp;#34;,&amp;#34;a+z@x.com&amp;#34;]) == [&amp;#34;a@x.com&amp;#34;]
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;machines-writing-for-machines-lessons-from-apljq&#34;&gt;Machines Writing for Machines: Lessons from APL/J/Q
&lt;/h2&gt;&lt;p&gt;Array languages like APL, J, and Q prioritize &lt;em&gt;terse, compositional semantics&lt;/em&gt; over conventional readability. Human legibility yields to density and mathematical regularity; small combinators compose into powerful pipelines. That philosophy maps well to model authorship:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Short, regular token sequences&lt;/strong&gt; are easier for models to predict and less prone to drift.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Uniform data semantics&lt;/strong&gt; (arrays everywhere) reduce branching surface and ambiguity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tacit/point-free style&lt;/strong&gt; (functions composed without naming arguments) compresses code and clarifies dataflow for static analyzers and LLMs alike.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An LLM-native language might borrow this spirit while remaining capable of being generated and manipulated by LLMs:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-arrayish&#34; data-lang=&#34;arrayish&#34;&gt;# all-pairs cosine similarity, top-5 per row
S ≔ normalize(X)             -- rows to unit length
C ≔ S · Sᵀ                  -- cosine matrix
Top5 ≔ take⟨5⟩ ∘ argsort⟨desc⟩(C, axis=cols)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The goal isn’t to replace clarity with glyphs; it’s to optimize the code’s statistical and algebraic structure for machine generation and analysis.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-coming-shift&#34;&gt;The Coming Shift
&lt;/h2&gt;&lt;p&gt;Just as previous generations of languages emerged in response to evolving needs, LLM-driven development will demand its own paradigms. The programming languages of the near future will be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Created &lt;em&gt;for&lt;/em&gt; LLMs to generate.&lt;/li&gt;
&lt;li&gt;Optimized for the ergonomics of machine authorship rather than human authorship.&lt;/li&gt;
&lt;li&gt;Built with native features to address the unique failure modes of AI-generated code.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can expect at least one such language to dominate a niche-perhaps in rapid prototyping, automated data processing, or even full-stack application generation-within the next decade.&lt;/p&gt;
&lt;p&gt;The next wave of programming language evolution won’t be about making life easier for human programmers alone. It will be about making life easier for our non-human collaborators.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
